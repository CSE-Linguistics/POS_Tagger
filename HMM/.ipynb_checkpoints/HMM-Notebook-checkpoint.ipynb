{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9595177670194431\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from nltk.corpus import brown\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def HMM():\n",
    "    tagged_data = []\n",
    "    train_corpus = []\n",
    "    test_corpus = []\n",
    "    all_tags = []\n",
    "    all_words = set()\n",
    "    tags_to_num = {}\n",
    "    num_to_tags = {}\n",
    "    transition = {}\n",
    "    emission = {}\n",
    "    \n",
    "    #Preprocess dataset\n",
    "    def __init__(self, train_test_ratio = 0.2):\n",
    "        tagged_sentences = brown.tagged_sents(tagset='universal')\n",
    "        for i in range(len(tagged_sentences)):\n",
    "            sent = []\n",
    "            sent.append((\"^\", \"^\"))\n",
    "            sent = sent + tagged_sentences[i]\n",
    "            tagged_data.append(sent)\n",
    "    \n",
    "    #Split dataset into train and test data\n",
    "    def split(train_test_ratio = 0.2):\n",
    "        train_corpus, test_corpus = train_test_split(tagged_data, test_size = 0.2)\n",
    "    \n",
    "    #Obtain other parameters like transition and emission probabilities\n",
    "    def extract_prob():    \n",
    "        #All tags in the train corpus\n",
    "        for sent in train_corpus:\n",
    "            for (w, t) in sent:\n",
    "                if t not in all_tags:\n",
    "                    all_tags.append(t)\n",
    "\n",
    "        #Count frequency for each tag\n",
    "        tags_freq = {}\n",
    "        for t in all_tags:\n",
    "            tags_freq[t] = 0\n",
    "        for sent in train_corpus:\n",
    "            for (w, t) in sent:\n",
    "                tags_freq[t] += 1\n",
    "\n",
    "        #tag to number and vice-versa\n",
    "        for i in range(len(all_tags)):\n",
    "            tags_to_num[all_tags[i]] = i\n",
    "            num_to_tags[i] = all_tags[i]\n",
    "\n",
    "        #All possible words in the set\n",
    "        for sent in train_corpus:\n",
    "            for (w, t) in train_corpus:\n",
    "                all_words.add(w)\n",
    "\n",
    "        #Count frequency for each (word, tag)\n",
    "        word_tag_freq = {}\n",
    "        for w in all_words:\n",
    "            for t in all_tags:\n",
    "                word_tag_freq[(w, t)] = 0\n",
    "        for sent in train_corpus:\n",
    "            for (w, t) in sent:\n",
    "                word_tag_freq[(w, t)] += 1\n",
    "\n",
    "        #evaluate transition  counts\n",
    "        for t1 in all_tags:\n",
    "            for t2 in all_tags:\n",
    "                transition[(t1, t2)] = 0\n",
    "        for sent in train_corpus:\n",
    "            for i in range(len(sent)-1):\n",
    "                transition[(sent[i][1], sent[i+1][1])] += 1\n",
    "\n",
    "        #evaluate transition and emission probabilities\n",
    "        for (t1, t2) in transition.keys():\n",
    "            transition[(t1, t2)] /= tags_freq[t1]\n",
    "        for (w, t) in word_tag_freq.keys():\n",
    "            emission[(w, t)] = word_tag_freq[(w, t)]/tags_freq[t]\n",
    "    \n",
    "    #Implements Viterbi Algorithm\n",
    "    def viterbi(sentence):\n",
    "        #returns predicted tag sequence\n",
    "        len_sent = len(sentence)\n",
    "        len_tagset = len(all_tags)\n",
    "\n",
    "        #SEQSCORE and BACKPTR arrays\n",
    "        SEQSCORE = [[0 for i in range(len_sent)] for j in range(len_tagset)]\n",
    "        BACKPTR =  [[0 for i in range(len_sent)] for j in range(len_tagset)]\n",
    "\n",
    "        null_tag = tags_to_num[\"^\"]\n",
    "        #initialise the null tag\n",
    "        SEQSCORE[null_tag][0] = 1\n",
    "\n",
    "        for i in range(1, len_sent):#Corresponds to a given word sentence[i]\n",
    "            for cidx, ctag in enumerate(all_tags):#Ending at current tag all_tags[j]\n",
    "\n",
    "                optimal_prob = 0 #Includes transitional probabilites\n",
    "                optimal_tag = 0\n",
    "\n",
    "                for pidx, ptag in enumerate(all_tags):#Previous tag all_tags[k]\n",
    "                    prob_k_j_i = SEQSCORE[pidx][i-1]*transition[(ptag, ctag)]\n",
    "                    if prob_k_j_i > optimal_prob:\n",
    "                        optimal_prob = prob_k_j_i\n",
    "                        optimal_tag = pidx\n",
    "\n",
    "                if sentence[i] in all_words:\n",
    "                    SEQSCORE[cidx][i] = optimal_prob*emission[(sentence[i], ctag)]\n",
    "                else:\n",
    "                    SEQSCORE[cidx][i] = optimal_prob\n",
    "                BACKPTR[cidx][i] = optimal_tag\n",
    "\n",
    "        #Sequence identification step\n",
    "        CT = 0\n",
    "        optimal_prob = 0\n",
    "        for i in range(len_tagset):\n",
    "            if SEQSCORE[i][len_sent-1]>optimal_prob:\n",
    "                optimal_prob = SEQSCORE[i][len_sent-1]\n",
    "                CT = i\n",
    "\n",
    "        pred_tags = [CT for i in range(len_sent)]\n",
    "        for i in reversed(range(len_sent-1)):\n",
    "            pred_tags[i] = BACKPTR[pred_tags[i+1]][i+1]\n",
    "\n",
    "        pred_tags = [num_to_tags[idx] for idx in pred_tags]\n",
    "        return pred_tags\n",
    "\n",
    "\n",
    "# #Load tag corpus and add the sentence beginner tag\n",
    "# def process_load():\n",
    "#     tagged_sentences = brown.tagged_sents(tagset='universal')\n",
    "#     tagged_data = []\n",
    "#     for i in range(len(tagged_sentences)):\n",
    "#         sentence = []\n",
    "#         sentence.append((\"^\", \"^\"))\n",
    "#         sentence = sentence + tagged_sentences[i]\n",
    "#         tagged_data.append(sentence)\n",
    "#     return tagged_data\n",
    "\n",
    "# #Return the emission and transition probabilities\n",
    "# def extract_prob(tagged_data):\n",
    "    \n",
    "#     #All tags in the set\n",
    "#     all_tags = []\n",
    "#     for sentence in tagged_data:\n",
    "#         for (word, tag) in sentence:\n",
    "#             if tag not in all_tags:\n",
    "#                 all_tags.append(tag)\n",
    "    \n",
    "#     #Count frequency for each tag\n",
    "#     tags_freq = {}\n",
    "#     for tag in all_tags:\n",
    "#         tags_freq[tag] = 0\n",
    "#     for sentence in tagged_data:\n",
    "#         for (word, tag) in sentence:\n",
    "#             tags_freq[tag] += 1\n",
    "    \n",
    "#     #tag to number and vice-versa\n",
    "#     tags_to_num = {}\n",
    "#     num_to_tags = {}\n",
    "#     for i in range(len(all_tags)):\n",
    "#         tags_to_num[all_tags[i]] = i\n",
    "#         num_to_tags[i] = all_tags[i]\n",
    "        \n",
    "#     #All possible words in the set\n",
    "#     all_words = set()\n",
    "#     for sentence in tagged_data:\n",
    "#         for (word, tag) in sentence:\n",
    "#             all_words.add(word)\n",
    "            \n",
    "#     #Count frequency for each (word, tag)\n",
    "#     word_tag_freq = {}\n",
    "#     for w in all_words:\n",
    "#         for t in all_tags:\n",
    "#             word_tag_freq[(w, t)] = 0\n",
    "#     for sentence in tagged_data:\n",
    "#         for (w, t) in sentence:\n",
    "#             word_tag_freq[(w, t)] += 1\n",
    "    \n",
    "#     #evaluate transition  counts\n",
    "#     transition = {}\n",
    "#     for t1 in all_tags:\n",
    "#         for t2 in all_tags:\n",
    "#             transition[(t1, t2)] = 0\n",
    "#     for sentence in tagged_data:\n",
    "#         for i in range(len(sentence)-1):\n",
    "#             transition[(sentence[i][1], sentence[i+1][1])] += 1\n",
    "    \n",
    "#     #evaluate transition and emission probabilities\n",
    "#     emission = {}\n",
    "#     for (t1, t2) in transition.keys():\n",
    "#         transition[(t1, t2)] /= tags_freq[t1]\n",
    "#     for (w, t) in word_tag_freq.keys():\n",
    "#         emission[(w, t)] = word_tag_freq[(w, t)]/tags_freq[t]\n",
    "#     return transition, emission, all_words, all_tags, tags_to_num, num_to_tags\n",
    "\n",
    "# #Implements Viterbi Algorithm\n",
    "# def viterbi(sentence, transition, emission, all_words, all_tags, tags_to_num, num_to_tags):\n",
    "#     #returns predicted tag sequence\n",
    "#     len_sent = len(sentence)\n",
    "#     len_tagset = len(all_tags)\n",
    "    \n",
    "#     #SEQSCORE and BACKPTR arrays\n",
    "#     SEQSCORE = [[0 for i in range(len_sent)] for j in range(len_tagset)]\n",
    "#     BACKPTR =  [[0 for i in range(len_sent)] for j in range(len_tagset)]\n",
    "    \n",
    "#     null_tag = tags_to_num[\"^\"]\n",
    "#     #initialise the null tag\n",
    "#     SEQSCORE[null_tag][0] = 1\n",
    "    \n",
    "#     for i in range(1, len_sent):#Corresponds to a given word sentence[i]\n",
    "#         for cidx, ctag in enumerate(all_tags):#Ending at current tag all_tags[j]\n",
    "            \n",
    "#             optimal_prob = 0 #Includes transitional probabilites\n",
    "#             optimal_tag = 0\n",
    "            \n",
    "#             for pidx, ptag in enumerate(all_tags):#Previous tag all_tags[k]\n",
    "#                 prob_k_j_i = SEQSCORE[pidx][i-1]*transition[(ptag, ctag)]\n",
    "#                 if prob_k_j_i > optimal_prob:\n",
    "#                     optimal_prob = prob_k_j_i\n",
    "#                     optimal_tag = pidx\n",
    "            \n",
    "#             if sentence[i] in all_words:\n",
    "#                 SEQSCORE[cidx][i] = optimal_prob*emission[(sentence[i], ctag)]\n",
    "#             else:\n",
    "#                 SEQSCORE[cidx][i] = optimal_prob\n",
    "#             BACKPTR[cidx][i] = optimal_tag\n",
    "                \n",
    "#     #Sequence identification step\n",
    "#     CT = 0\n",
    "#     optimal_prob = 0\n",
    "#     for i in range(len_tagset):\n",
    "#         if SEQSCORE[i][len_sent-1]>optimal_prob:\n",
    "#             optimal_prob = SEQSCORE[i][len_sent-1]\n",
    "#             CT = i\n",
    "    \n",
    "#     pred_tags = [CT for i in range(len_sent)]\n",
    "#     for i in reversed(range(len_sent-1)):\n",
    "#         pred_tags[i] = BACKPTR[pred_tags[i+1]][i+1]\n",
    "    \n",
    "#     pred_tags = [num_to_tags[idx] for idx in pred_tags]\n",
    "#     return pred_tags\n",
    "    \n",
    "    \n",
    "# tagged_corpus = process_load()\n",
    "# train_corpus, test_corpus = sklearn.model_selection.train_test_split(tagged_corpus, test_size = 0.2)\n",
    "# transition, emission, all_words, all_tags, tags_to_num, num_to_tags = extract_prob(train_corpus)\n",
    "\n",
    "count_accurate = 0\n",
    "count_total = 0\n",
    "for sentence in test_corpus:\n",
    "    untagged_sentence = []\n",
    "    for (w, t) in sentence:\n",
    "        untagged_sentence.append(w)\n",
    "    predicted_tags = viterbi(untagged_sentence, transition, emission, all_words, all_tags, tags_to_num, num_to_tags)\n",
    "    for i in range(len(sentence)):\n",
    "        if predicted_tags[i] == senten  ce[i][1]:\n",
    "            count_accurate += 1\n",
    "        count_total += 1\n",
    "print(count_accurate/count_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tagged corpora\n",
    "def load_data():\n",
    "\ttagged_sentences = nltk.corpus.brown.tagged_sents(tagset='universal')\n",
    "\ttagged_data = []\n",
    "\tfor i in range(len(tagged_sentences)):\n",
    "\t\ttagged_data.append((\"^\", \"^\"))\n",
    "\t\tfor w in tagged_sentences[i]:\n",
    "\t\t\ttagged_data.append(w)\n",
    "\treturn tagged_data\n",
    "# Find all unique tags and count for each tag\n",
    "def extract_prob(tagged_data):\n",
    "\ttags_all = []\n",
    "\tfor (word,tag) in tagged_data:\n",
    "\t\ttags_all.append(tag)\n",
    "\n",
    "\t(tagset, count) = np.unique(tags_all, return_counts=True)\n",
    "\tprint(tagset)\n",
    "\ttags_dict = {}\n",
    "\tfor i in range(len(tagset)):\n",
    "\t\ttags_dict[tagset[i]] = count[i]\n",
    "\n",
    "\t# Find all unique words\n",
    "\twords_all = []\n",
    "\tfor (w,t) in tagged_data:\n",
    "\t\twords_all.append(w)\n",
    "\twords = np.unique(words_all)\n",
    "\n",
    "\t# Find count of all (word, tag pairs)\n",
    "\tword_tag_freq = {}\n",
    "\tfor w in words:\n",
    "\t\tfor t in tagset:\n",
    "\t\t\tword_tag_freq[(w,t)] = 0\n",
    "\tfor (w,t) in tagged_data:\n",
    "\t\tword_tag_freq[(w,t)] += 1\n",
    "\n",
    "\t# Find count of one tag following another\n",
    "\t# t1 is the first tag, t2 is the following tag\n",
    "\ttransitions = {}\n",
    "\tfor t1 in tagset:\n",
    "\t\tfor t2 in tagset:\n",
    "\t\t\ttransitions[(t1,t2)] = 0\n",
    "\tfor i in range(len(tagged_data)-1):\n",
    "\t\ttransitions[(tagged_data[i][1], tagged_data[i+1][1])] += 1\n",
    "\n",
    "\t# Obtain the emission and transition probabilities\n",
    "\temission = word_tag_freq\n",
    "\tfor (w,t) in emission.keys():\n",
    "\t\temission[(w,t)] = emission[(w,t)]/tags_dict[t]\n",
    "\tfor (t1,t2) in transitions.keys():\n",
    "\t\ttransitions[(t1,t2)] = transitions[(t1,t2)]/tags_dict[t1]\n",
    "\treturn transitions, emission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
